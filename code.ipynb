{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40a46cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86e53d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b175bffc",
   "metadata": {},
   "source": [
    "<h3>ETAPA 0 - Leitura do CSV (com checagens)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbb1e4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "CSV_PATH = Path(\"input.csv\")\n",
    "\n",
    "assert CSV_PATH.exists(), f\"Arquivo não encontrado: {CSV_PATH.resolve()}\"\n",
    "\n",
    "# Tentativas de leitura com encodings comuns\n",
    "encodings_to_try = [\"utf-8\", \"utf-8-sig\", \"latin-1\"]\n",
    "last_err = None\n",
    "df = None\n",
    "for enc in encodings_to_try:\n",
    "    try:\n",
    "        df = pd.read_csv(CSV_PATH, encoding=enc)\n",
    "        print(f\"[OK] Lido com encoding: {enc}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        last_err = e\n",
    "\n",
    "if df is None:\n",
    "    raise RuntimeError(f\"Falha ao ler CSV. Último erro: {last_err}\")\n",
    "\n",
    "# Mostra um resumo rápido\n",
    "print(\"\\n[INFO] Formato do DataFrame:\", df.shape)\n",
    "print(\"[INFO] Primeiras colunas:\", list(df.columns[:10]))\n",
    "print(\"\\n[INFO] Amostra (5 linhas):\")\n",
    "print(df.head(5))\n",
    "\n",
    "# Helper para localizar colunas mesmo com sufixos (ex.: 'Type (Remove)')\n",
    "def find_col(cols, key):\n",
    "    key = key.lower()\n",
    "    for c in cols:\n",
    "        if key in c.lower():\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "# Detecta as colunas principais (só para conferência visual nesta etapa)\n",
    "col_type  = find_col(df.columns, \"Type\")\n",
    "col_env   = find_col(df.columns, \"Environment\")\n",
    "col_cr    = find_col(df.columns, \"CR\")\n",
    "col_size  = find_col(df.columns, \"Size\")\n",
    "\n",
    "print(\"\\n[DETECÇÃO DE COLUNAS]\")\n",
    "print(\"Type:\", col_type)\n",
    "print(\"Environment:\", col_env)\n",
    "print(\"CR:\", col_cr)\n",
    "print(\"Size:\", col_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe75c78",
   "metadata": {},
   "source": [
    "<h3>ETAPA 1 - Seleção de colunas essenciais</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10ee64c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Dicionário com nomes detectados (da etapa anterior)\n",
    "ESSENTIALS = {\n",
    "    \"type\": col_type,\n",
    "    \"env\": col_env,\n",
    "    \"cr\": col_cr,\n",
    "    \"size\": col_size\n",
    "}\n",
    "\n",
    "print(\"\\n[INFO] Colunas essenciais selecionadas:\")\n",
    "for k, v in ESSENTIALS.items():\n",
    "    print(f\"  {k:>8} -> {v}\")\n",
    "\n",
    "# Cria uma cópia só com essas colunas\n",
    "df_work = df[[v for v in ESSENTIALS.values() if v is not None]].copy()\n",
    "\n",
    "print(\"\\n[INFO] DataFrame de trabalho criado.\")\n",
    "print(\"[INFO] Formato:\", df_work.shape)\n",
    "print(\"[INFO] Colunas:\", list(df_work.columns))\n",
    "print(\"\\nPrévia:\")\n",
    "print(df_work.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8c8835",
   "metadata": {},
   "source": [
    "<h3>ETAPA 2 - Limpeza e transformação dos dados</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e94c9568",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "print(\"\\nETAPA 2 - Iniciando Limpeza e Transformação...\")\n",
    "\n",
    "# Funções Auxiliares\n",
    "def parse_cr(x):\n",
    "    # converter valores de CR para float\n",
    "    s = str(x).strip()\n",
    "    if s in (\"nan\", \"\", \"—\", \"-\", \"None\"):\n",
    "        return np.nan\n",
    "    if \"/\" in s:\n",
    "        try:\n",
    "            a, b = s.split(\"/\", 1)\n",
    "            return float(a) / float(b)\n",
    "        except:\n",
    "            pass\n",
    "    try:\n",
    "        return float(s)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def clean_text_basic(s):\n",
    "    # Remove parenteses e normaliza o caps\n",
    "    s = re.sub(r\"\\(.*?\\)\", \"\", str(s))\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip().title()\n",
    "\n",
    "def normalize_env(env_str):\n",
    "    # padronizar ambientes (environment)\n",
    "    parts = [p.strip().title() for p in str(env_str).split(\",\") if p.strip()]\n",
    "    if not parts:\n",
    "        return \"Unknown\"\n",
    "    seen, out = set(), []\n",
    "    for p in parts:\n",
    "        if p not in seen:\n",
    "            seen.add(p)\n",
    "            out.append(p)\n",
    "    return \", \".join(out)\n",
    "\n",
    "#Aplicando limpeza\n",
    "df_work[\"Type\"] = df_work[\"Type\"].apply(clean_text_basic)\n",
    "df_work[\"Size\"] =  df_work[\"Size\"].apply(clean_text_basic)\n",
    "df_work[\"Environment\"] = df_work[\"Environment\"].apply(normalize_env)\n",
    "df_work[\"CR_float\"] = df_work[\"CR\"].apply(parse_cr)\n",
    "\n",
    "#Tratando valores faltantes\n",
    "df_work[\"CR_float\"] = df_work[\"CR_float\"].fillna(0.0)\n",
    "df_work[\"Environment\"] = df_work[\"Environment\"].replace(\"\", \"Unknown\")\n",
    "df_work[\"Type\"] = df_work[\"Type\"].replace(\"\", \"Unknown\")\n",
    "\n",
    "\n",
    "# Eliminar duplicatas\n",
    "before = len(df_work)\n",
    "df_work.drop_duplicates(inplace=True)\n",
    "after = len(df_work)\n",
    "\n",
    "print(f\"[INFO] Duplicatas removidas: {before - after}\")\n",
    "print(\"[INFO] Visualização dos dados limpos:\")\n",
    "print(df_work.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7744f83",
   "metadata": {},
   "source": [
    "<h3>ETAPA 3 - Transformação (one-hot e multi-one-hot)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27949ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "print(\"\\nETAPA 3 - Iniciando Transformação (one-hot e multi-one-hot)...\")\n",
    "\n",
    "#ID estavel apos a limpeza\n",
    "df_work= df_work.reset_index(drop=True)\n",
    "df_work.insert(0, \"MonsterID\", df_work.index + 1)\n",
    "\n",
    "#One-hot de 'Type' e 'Size'\n",
    "type_dummies = pd.get_dummies(df_work[\"Type\"], prefix=\"Type\")\n",
    "size_dummies = pd.get_dummies(df_work[\"Size\"], prefix=\"Size\")\n",
    "\n",
    "#Multi-one-hot de 'Environment'\n",
    "known_envs = [\"Arctic\", \"Cave\" , \"Desert\", \"Dungeon\", \"Forest\", \"Hell\", \"Mountain\", \"Plains\", \"Sky\", \"Underground\", \"Urban\", \"Water\", \"Unknown\"]\n",
    "\n",
    "def env_one_hot(env_str):\n",
    "    envs = [e.strip() for e in env_str.split(\",\") if e.strip()]\n",
    "    return {f\"env_{e}\": int(e in envs) for e in known_envs}\n",
    "\n",
    "env_dummies = df_work[\"Environment\"].apply(env_one_hot).apply(pd.Series)\n",
    "\n",
    "features = pd.concat([\n",
    "    df_work[[\"MonsterID\", \"CR_float\"]],\n",
    "    type_dummies,\n",
    "    size_dummies,\n",
    "    env_dummies\n",
    "], axis=1)\n",
    "\n",
    "print(\"[INFO] Preview das features:\",features.shape)\n",
    "print(features.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cec9ed5",
   "metadata": {},
   "source": [
    "<h3>Exportação final</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4aa6b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ETAPA 4 - Exportação final...\n",
      "[INFO] Arquivos exportados:\n",
      "  - Limpo -> /home/arthur/UFSJ/IA/Kraken/outputs/monsters_clean.csv\n",
      "  - Treino -> /home/arthur/UFSJ/IA/Kraken/outputs/monsters_train.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nETAPA 4 - Exportação final...\")\n",
    "\n",
    "OUT_DIR = Path(\"outputs\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "monsters_clean_path = OUT_DIR / \"monsters_clean.csv\"\n",
    "monsters_train_path = OUT_DIR / \"monsters_train.csv\"\n",
    "\n",
    "df_clean = df_work[[\"MonsterID\", \"Type\", \"Size\", \"Environment\",\"CR\", \"CR_float\"]]\n",
    "df_clean.to_csv(monsters_clean_path, index=False)\n",
    "\n",
    "features.to_csv(monsters_train_path, index=False)\n",
    "\n",
    "print(f\"[INFO] Arquivos exportados:\")\n",
    "print(f\"  - Limpo -> {monsters_clean_path.resolve()}\")\n",
    "print(f\"  - Treino -> {monsters_train_path.resolve()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
